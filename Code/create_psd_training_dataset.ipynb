{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create PSD (power spectral density) dataset in H5 format. \n",
    "Load required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import h5py\n",
    "import scipy.io\n",
    "import matlab_helpers as mh \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all txt files used to train/test the feature-based model. Create a dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_prevlist_dir = 'D:/Dropbox/Pitts_files/ShiTongShare/ANcode_GPlynx/GPlynx_list_cluster/'\n",
    "cluster_datadir_name = '/ix/ssadagopan/sap245/GPfiles_MIFanalysis/cog_gp/vIHC/level65_dBspl_clean/'\n",
    "all_call_dirnames = [x for x in os.listdir(root_prevlist_dir)]\n",
    "# all_call_dirnames= all_call_dirnames[1:]\n",
    "print(all_call_dirnames)\n",
    "\n",
    "files_in_test = []\n",
    "for cur_call_dir in all_call_dirnames:\n",
    "    with open(root_prevlist_dir + cur_call_dir + '/inclass_test.txt') as f_in:\n",
    "        cur_lines = f_in.readlines()\n",
    "        cur_lines = [item.replace(item[:1+item.rfind('/')], '') for item in cur_lines]\n",
    "        cur_lines = [item.replace('\\n', '') for item in cur_lines]\n",
    "        files_in_test = files_in_test + cur_lines\n",
    "    with open(root_prevlist_dir + cur_call_dir + '/outclass_test.txt') as f_out:\n",
    "        cur_lines = f_out.readlines()\n",
    "        cur_lines = [item.replace(item[:1+item.rfind('/')], '') for item in cur_lines]\n",
    "        cur_lines = [item.replace('\\n', '') for item in cur_lines]\n",
    "        files_in_test = files_in_test + cur_lines\n",
    "    \n",
    "files_in_test = list(set(files_in_test)) # Only keep unique entries \n",
    "print(f\"len lines= {len(files_in_test)}, first five lines = \\n {files_in_test[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the names of all mat-files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_matdata_dir = 'D:/Dropbox/Python/MLmodels/Datasets/psd_gp_vIHC_mat/level65_dBspl_clean/'\n",
    "valid_datadirs = ['Chut', 'HighWhistle', 'Rumble', 'Tchatter', 'Wheek', 'Whine']\n",
    "\n",
    "all_files_nameonly= []\n",
    "all_files_fullname= []\n",
    "\n",
    "for cur_call_dir in valid_datadirs:\n",
    "    cur_call_path = root_matdata_dir + cur_call_dir + '/' \n",
    "    # print(cur_call_path)\n",
    "    cur_dir_files = [f for f in os.listdir(cur_call_path) if os.path.isfile(os.path.join(cur_call_path, f))]\n",
    "    all_files_nameonly = all_files_nameonly + cur_dir_files\n",
    "    all_files_fullname = all_files_fullname + [cur_call_path + f for f in cur_dir_files]\n",
    "    \n",
    "\n",
    "files_in_test_prefixed = ['psd_' + item for item in files_in_test]\n",
    "# test_data_inds = [i for i, e in enumerate(files_in_test_prefixed) if e in set(all_files_nameonly)]\n",
    "test_data_inds = [i for i, e in enumerate(files_in_test_prefixed) if e in set(all_files_nameonly)]\n",
    "train_data_inds = [i for i in np.arange(len(all_files_fullname)) if i not in test_data_inds] \n",
    "\n",
    "print(f\"type={type(test_data_inds)},len={len(train_data_inds)} | first three elements = {train_data_inds[0:3]}\")\n",
    "\n",
    "train_data_list = [all_files_fullname[i] for i in train_data_inds]\n",
    "test_data_list = [all_files_fullname[i] for i in test_data_inds]\n",
    "\n",
    "print(f\"--> all_files_fullname: len lines= {len(all_files_fullname)}, first line ={all_files_fullname[0]}\")\n",
    "print(f\"--> all_files_nameonly: len lines= {len(all_files_nameonly)}, first line = {all_files_nameonly[0]}\")\n",
    "print(f\"--> files_in_test_prefixed: len lines= {len(files_in_test_prefixed)}, first line = {files_in_test_prefixed[0]}\")\n",
    "print(f\"--> train_data_list: len lines= {len(train_data_list)}, first line = {train_data_list[0]}\")\n",
    "\n",
    "out_train_txt_fname = 'D:/Dropbox/Python/MLmodels/Datasets/train_data_list.txt'\n",
    "with open(out_train_txt_fname, 'w') as f:\n",
    "    for line in train_data_list:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "out_test_txt_fname = 'D:/Dropbox/Python/MLmodels/Datasets/test_data_list.txt'\n",
    "with open(out_test_txt_fname, 'w') as f:\n",
    "    for line in test_data_list:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Mat files and then create .h5 files for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_trainfile = 'D:/Dropbox/Python/MLmodels/Datasets/train_data.h5'\n",
    "data_psd_train_x = []\n",
    "pre_search_str = 'clean/'\n",
    "post_search_str = '/psd_'\n",
    "\n",
    "for fName in train_data_list:\n",
    "    data = mh.loadmat(fName)\n",
    "    data_psd_train_x.append(data[\"psd_data\"][\"psd\"]) \n",
    "    \n",
    "data_label_train_name = [item[item.rfind(pre_search_str)+len(pre_search_str):item.rfind(post_search_str)] for item in train_data_list]\n",
    "data_label_train_y = len(all_call_dirnames)*np.ones((len(data_psd_train_x),1))\n",
    "unq_vals, unq_counts = np.unique(data_label_train_y, return_counts=True)\n",
    "print(dict(zip(unq_vals,unq_counts)))\n",
    "\n",
    "for ind, cur_call in zip(np.arange(len(data_label_train_name)),data_label_train_name):\n",
    "    if cur_call in all_call_dirnames: \n",
    "        data_label_train_y[ind,0] = all_call_dirnames.index(cur_call)\n",
    "\n",
    "data_psd_train_x = np.array(data_psd_train_x)\n",
    "data_label_train_y = np.array(data_label_train_y).astype(int)\n",
    "print(f\"data_psd_train_x={type(data_psd_train_x)}&{len(data_psd_train_x)},data_label_train_y={type(data_label_train_y)}&{data_label_train_y.shape},\")\n",
    "\n",
    "hf = h5py.File(out_trainfile, \"w\")\n",
    "hf.close()\n",
    "\n",
    "if (not os.path.exists(out_trainfile)):\n",
    "    print(\"Saving file\" + out_trainfile)\n",
    "    hf = h5py.File(out_trainfile, \"w\")\n",
    "    hf.create_dataset('data_psd_train_x',data=data_psd_train_x)\n",
    "    hf.create_dataset('data_label_train_y',data=data_label_train_y)\n",
    "    hf.close()\n",
    "else: \n",
    "    print(\"File (\" + out_trainfile + \") already exists\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Mat files and then create .h5 files for testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_testfile = 'D:/Dropbox/Python/MLmodels/Datasets/test_data.h5'\n",
    "data_psd_test_x = []\n",
    "\n",
    "for fName in test_data_list:\n",
    "    data = mh.loadmat(fName)\n",
    "    data_psd_test_x.append(data[\"psd_data\"][\"psd\"]) \n",
    "    \n",
    "data_label_test_name = [item[item.rfind(pre_search_str)+len(pre_search_str):item.rfind(post_search_str)] for item in test_data_list]\n",
    "data_label_test_y = len(all_call_dirnames)*np.ones((len(data_psd_test_x),1))\n",
    "\n",
    "for ind, cur_call in zip(np.arange(len(data_label_test_name)),data_label_test_name):\n",
    "    if cur_call in all_call_dirnames: \n",
    "        data_label_test_y[ind,0] = all_call_dirnames.index(cur_call)\n",
    "\n",
    "data_psd_test_x = np.array(data_psd_test_x)\n",
    "data_label_test_y = np.array(data_label_test_y).astype(int)\n",
    "print(f\"data_psd_test_x={type(data_psd_test_x)}&{len(data_psd_test_x)},data_label_train_y={type(data_label_test_y)}&{data_label_test_y.shape},\")\n",
    "\n",
    "if not os.path.exists(out_testfile):\n",
    "    print(\"Saving file\" + out_testfile)\n",
    "    hf = h5py.File(out_testfile, \"w\")\n",
    "    hf.create_dataset('data_psd_test_x',data=data_psd_test_x)\n",
    "    hf.create_dataset('data_label_test_y',data=data_label_test_y)\n",
    "    hf.close()\n",
    "else: \n",
    "    print(\"File (\" + out_testfile + \") already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9901b0122a0103a0e954f0712b860e20e1581f681d96d994631537742cd9f65d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
