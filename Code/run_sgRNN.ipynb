{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import h5py \n",
    "import matlab_helpers as mh \n",
    "# from sklearn.model_selection import test_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "import copy \n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset \n",
    "Steps are \n",
    "1. Load training data \n",
    "2. To use the parallel training, need to use the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_out_dir = '../Datasets/sgRNN_vIHC/PyData/'\n",
    "train_dataset = np.load(root_out_dir + 'sgRNN_data_train_file.npz', allow_pickle=True)\n",
    "print(train_dataset.files)\n",
    "\n",
    "X_train = train_dataset[\"data_sgRNN_x\"][:] # your train set features\n",
    "y_train = to_categorical(np.ravel(np.array(train_dataset[\"data_label_y\"][:]))) # your train set labels\n",
    "\n",
    "# Preprocess to make the data the same length \n",
    "train_len_pre = np.array([item.shape[1] for item in X_train])\n",
    "min_val = np.array([np.min(np.ravel(item)) for item in X_train])\n",
    "max_val = np.array([np.max(np.ravel(item)) for item in X_train])\n",
    "\n",
    "max_len = train_len_pre.max()\n",
    "X_train = [np.concatenate([item, -(2**7)*np.ones((item.shape[0], max_len-item.shape[1]))],axis=1) for item in X_train]\n",
    "train_len_post = np.array([item.shape[1] for item in X_train])\n",
    "X_train = np.asarray(X_train, dtype=np.int8)\n",
    "X_train = np.transpose(X_train, (0, 2, 1))\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# If we want to train using clean calls only, then take the last 1203 points from the training dataset\n",
    "X_train_clean = X_train[-1203:,:,:]\n",
    "y_train_clean = y_train[-1203:,:]\n",
    "\n",
    "test_dataset = np.load(root_out_dir + 'sgRNN_data_test_snrInf.npz', allow_pickle=True)\n",
    "print(test_dataset)\n",
    "X_test = test_dataset[\"data_sgRNN_x\"][:] # your train set features\n",
    "X_test = [np.concatenate([item, -(2**7)*np.ones((item.shape[0], max_len-item.shape[1]))],axis=1) for item in X_test]\n",
    "X_test = np.asarray(X_test, dtype=np.int8)\n",
    "X_test = np.transpose(X_test, (0, 2, 1))\n",
    "y_test = to_categorical(np.ravel(np.array(test_dataset[\"data_label_y\"][:]))) # your train set labels\n",
    "\n",
    "print(f\"Type: X_train={type(X_train)}, y_train={type(y_train)}\")\n",
    "print(f\"Shapes: X_train={X_train.shape}, y_train={y_train.shape},X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "unq_vals, unq_counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unq_vals,unq_counts)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an RNN to fit using noisy data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional\n",
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "\n",
    "RNNmodel_noisy = tf.keras.models.Sequential()\n",
    "RNNmodel_noisy.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(73, 67)))\n",
    "RNNmodel_noisy.add(Bidirectional(LSTM(20, return_sequences=False)))\n",
    "RNNmodel_noisy.add(Dense(5, activation='softmax'))\n",
    "\n",
    "RNNmodel_noisy.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "RNNmodel_noisy.summary()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
    "history_noisy = RNNmodel_noisy.fit(train_dataset, epochs=100, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an RNN to fit using clean data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, Bidirectional\n",
    "tf.random.set_seed(1234)  # applied to achieve consistent results\n",
    "\n",
    "RNNmodel_clean = tf.keras.models.Sequential()\n",
    "RNNmodel_clean.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(73, 67)))\n",
    "RNNmodel_clean.add(Bidirectional(LSTM(20, return_sequences=False)))\n",
    "RNNmodel_clean.add(Dense(5, activation='softmax'))\n",
    "\n",
    "RNNmodel_clean.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "RNNmodel_clean.summary()\n",
    "\n",
    "train_dataset_clean = tf.data.Dataset.from_tensor_slices((X_train_clean, y_train_clean)).batch(32)\n",
    "history_clean = RNNmodel_clean.fit(train_dataset_clean, epochs=100, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check output of NN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_NN = np.argmax(RNNmodel_noisy.predict(X_train), axis=1)\n",
    "y_test_pred_NN = np.argmax(RNNmodel_noisy.predict(X_test), axis=1)\n",
    "y_train_numeric= np.argmax(y_train, axis=1)\n",
    "y_test_numeric= np.argmax(y_test, axis=1)\n",
    "\n",
    "print(f\"Unique values in y_train_pred_NN={np.unique(y_train_pred_NN)}\")\n",
    "print(f\"Unique values in y_test_pred_NN={np.unique(y_test_pred_NN)}\")\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(4,4))\n",
    "ax[0].plot(y_train_numeric)\n",
    "ax[1].plot(y_train_numeric-y_train_pred_NN, color = \"orangered\")\n",
    "ax[1].set_xlabel(\"Call numbers\")\n",
    "\n",
    "print(f\"Training accuracy={np.sum(y_train_numeric==y_train_pred_NN)/len(y_train)}\\n Testing accuracy={np.sum(y_test_numeric==y_test_pred_NN)/len(y_test_numeric)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test in different SNRs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_snrs= np.arange(-20.0, 11, 5)\n",
    "all_snrs= np.append(all_snrs, np.inf)\n",
    "\n",
    "accuracy_vs_snr_noisyRNN = np.zeros((all_snrs.shape))\n",
    "accuracy_vs_snr_cleanRNN = np.zeros((all_snrs.shape))\n",
    "\n",
    "for snr_value,iter in zip(all_snrs,range(all_snrs.shape[0])):\n",
    "    if np.isinf(snr_value):\n",
    "        out_sgRNN_data_test_file= root_out_dir + 'sgRNN_data_test_snrInf.npz'\n",
    "    else: \n",
    "        out_sgRNN_data_test_file= root_out_dir + 'sgRNN_data_test_snr' + str(np.int_(snr_value)) + '.npz'\n",
    "    print(out_sgRNN_data_test_file)\n",
    "\n",
    "    test_dataset = np.load(out_sgRNN_data_test_file, allow_pickle=True)\n",
    "    print(test_dataset)\n",
    "    X_test = test_dataset[\"data_sgRNN_x\"][:] # your train set features\n",
    "    X_test = [np.concatenate([item, -(2**7)*np.ones((item.shape[0], max_len-item.shape[1]))],axis=1) for item in X_test]\n",
    "    X_test = np.asarray(X_test, dtype=np.int8)\n",
    "    X_test = np.transpose(X_test, (0, 2, 1))\n",
    "    y_test = np.ravel(np.array(test_dataset[\"data_label_y\"][:])) # your train set labels\n",
    "    \n",
    "    y_test_pred_NN_noisy = np.argmax(RNNmodel_noisy.predict(X_test), axis=1)\n",
    "    y_test_pred_NN_clean = np.argmax(RNNmodel_clean.predict(X_test), axis=1)\n",
    "    \n",
    "    accuracy_vs_snr_noisyRNN[iter] = np.sum(y_test==y_test_pred_NN_noisy)/len(y_test)\n",
    "    accuracy_vs_snr_cleanRNN[iter] = np.sum(y_test==y_test_pred_NN_clean)/len(y_test)\n",
    "    print(f\"Testing accuracy: Noisy = {accuracy_vs_snr_noisyRNN[iter]}, clean = {y_test_pred_NN_clean[iter]}\\n\")\n",
    "\n",
    "plot_snr= copy.deepcopy(all_snrs)\n",
    "plot_snr[np.isinf(plot_snr)] = 15\n",
    "plt.plot(plot_snr, accuracy_vs_snr_noisyRNN, label=\"Noise-trained\")\n",
    "plt.plot(plot_snr, accuracy_vs_snr_cleanRNN, label=\"Clean-trained\")\n",
    "plt.xlabel(\"SNR (dB)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"RNN/LSTM\")\n",
    "plt.legend()\n",
    "\n",
    "out_sgRNN_classify_file = root_out_dir + 'sgRNN_classify_out'\n",
    "np.savez(out_sgRNN_classify_file + '.npz', all_snrs=all_snrs, accuracy_vs_snr_noisyRNN=accuracy_vs_snr_noisyRNN, accuracy_vs_snr_cleanRNN=accuracy_vs_snr_cleanRNN)\n",
    "mat_dict= {\"all_snrs\":all_snrs, \"accuracy_vs_snr_noisyRNN\":accuracy_vs_snr_noisyRNN, \"accuracy_vs_snr_cleanRNN\":accuracy_vs_snr_cleanRNN}\n",
    "savemat(out_sgRNN_classify_file + '.mat', mat_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9901b0122a0103a0e954f0712b860e20e1581f681d96d994631537742cd9f65d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
