{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import h5py\n",
    "import scipy.io\n",
    "import matlab_helpers as mh \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read unique files and create training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import int8\n",
    "\n",
    "root_in_dir= '../Datasets/mps_gp_sgram/level65_dBspl_ds_int8/'\n",
    "root_out_dir= '../Datasets/mps_gp_sgram/PyData/'\n",
    "if not os.path.exists(root_out_dir):\n",
    "    os.makedirs(root_out_dir)\n",
    "\n",
    "out_dsMPS_data_train_file= root_out_dir + 'dsMPS_data_train_file.h5'\n",
    "out_dsMPS_data_train_listfile= root_out_dir + 'dsMPS_train_list.txt'\n",
    "out_dsMPS_data_test_listfile= root_out_dir + 'dsMPS_test_file.txt'\n",
    "\n",
    "valid_datadirs = ['Chut', 'HighWhistle', 'Rumble', 'Tchatter', 'Wheek', 'Whine']\n",
    "calls2use = ['Chut', 'Rumble', 'Wheek', 'Whine']\n",
    "\n",
    "uniq_files_all_calls = []\n",
    "for cur_call_dir in valid_datadirs:\n",
    "    cur_call_path = root_in_dir + cur_call_dir + '/' \n",
    "    cur_dir_files = [f for f in os.listdir(cur_call_path) if os.path.isfile(os.path.join(cur_call_path, f))]\n",
    "    filenames= [item[0:item.rfind('_snr')] for item in cur_dir_files]\n",
    "    unq_filenames= list(set(filenames))\n",
    "    uniq_files_all_calls = uniq_files_all_calls + unq_filenames\n",
    "    print(f\"total={len(filenames)} | unique={len(unq_filenames)} | math = {len(filenames)/len(unq_filenames)}\")\n",
    "\n",
    "print(f\"{len(uniq_files_all_calls)} total unique files\")\n",
    "\n",
    "training_files, testing_files, training_inds, testing_inds = \\\n",
    "    train_test_split(uniq_files_all_calls, np.arange(len(uniq_files_all_calls)), test_size= 0.25, random_state=1)\n",
    "\n",
    "with open(out_dsMPS_data_train_listfile, 'w') as f:\n",
    "    for line in training_files:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open(out_dsMPS_data_test_listfile, 'w') as f:\n",
    "    for line in testing_files:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "all_snrs= np.arange(-20, 11, 5)\n",
    "all_snrs= np.append(all_snrs, np.inf)\n",
    "snr_training_weights= np.flip(len(all_snrs)-np.arange(len(all_snrs)))/len(all_snrs)\n",
    "\n",
    "data_dsMPS_x = []\n",
    "data_label_y = []\n",
    "data_filename = []\n",
    "\n",
    "for snr_value,cur_weight in zip(all_snrs, snr_training_weights):\n",
    "    print(f\"snr_value={snr_value} with weight = {cur_weight}\")\n",
    "    if cur_weight<1:\n",
    "        cur_snr_train_inds, _ = train_test_split(np.arange(len(training_inds)), test_size= 1-cur_weight, random_state=1)\n",
    "    else:\n",
    "        cur_snr_train_inds = training_inds\n",
    "\n",
    "    for ind in cur_snr_train_inds:\n",
    "        cur_file = uniq_files_all_calls[ind]\n",
    "        cur_call = cur_file[:cur_file.find('_')]\n",
    "        if np.isinf(snr_value):\n",
    "            cur_filename= root_in_dir + cur_call + '/' + cur_file + '_snrInf.mat'  # because matlab uses Inf not inf \n",
    "        else:\n",
    "            cur_filename= root_in_dir + cur_call + '/' + cur_file + '_snr' + str(np.int_(snr_value)).replace('-', 'm') + '.mat'\n",
    "\n",
    "        new_data = mh.loadmat(cur_filename)\n",
    "        data_dsMPS_x.append(new_data[\"mps_struct\"][\"mps_pow_dB\"])\n",
    "\n",
    "        if cur_call in calls2use:\n",
    "            data_label_y.append(calls2use.index(cur_call))\n",
    "        else:\n",
    "            data_label_y.append(len(calls2use))\n",
    "        \n",
    "        data_filename.append(cur_filename)\n",
    "\n",
    "        print(cur_filename)\n",
    "\n",
    "data_dsMPS_x = np.array(data_dsMPS_x).astype(int8)\n",
    "data_label_y = np.array(data_label_y).astype(int)\n",
    "\n",
    "if (not os.path.exists(out_dsMPS_data_train_file)):\n",
    "    print(\"Saving file\" + out_dsMPS_data_train_file)\n",
    "    hf = h5py.File(out_dsMPS_data_train_file, \"w\")\n",
    "    hf.create_dataset('data_dsMPS_x',data=data_dsMPS_x)\n",
    "    hf.create_dataset('data_label_y',data=data_label_y)\n",
    "    hf.close()\n",
    "else: \n",
    "    print(\"File (\" + out_dsMPS_data_train_file + \") already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test dataset (for different SNRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snr_value in all_snrs:\n",
    "    print(f\"snr_value={snr_value} with weight = {cur_weight}\")\n",
    "    data_dsMPS_x = []\n",
    "    data_label_y = []\n",
    "    data_filename = []\n",
    "\n",
    "    if np.isinf(snr_value):\n",
    "        out_dsMPS_data_test_file= root_out_dir + 'dsMPS_data_test_snrInf.h5'\n",
    "    else: \n",
    "        out_dsMPS_data_test_file= root_out_dir + 'dsMPS_data_test_snr' + str(np.int_(snr_value)) + '.h5'\n",
    "    print(out_dsMPS_data_test_file)\n",
    "    if (not os.path.exists(out_dsMPS_data_test_file)):\n",
    "\n",
    "        for ind in testing_inds:\n",
    "            cur_file = uniq_files_all_calls[ind]\n",
    "            cur_call = cur_file[:cur_file.find('_')]\n",
    "            if np.isinf(snr_value):\n",
    "                cur_filename= root_in_dir + cur_call + '/' + cur_file + '_snrInf.mat'  # because matlab uses Inf not inf \n",
    "            else:\n",
    "                cur_filename= root_in_dir + cur_call + '/' + cur_file + '_snr' + str(np.int_(snr_value)).replace('-', 'm') + '.mat'\n",
    "\n",
    "            new_data = mh.loadmat(cur_filename)\n",
    "            data_dsMPS_x.append(new_data[\"mps_struct\"][\"mps_pow_dB\"])\n",
    "\n",
    "            if cur_call in calls2use:\n",
    "                data_label_y.append(calls2use.index(cur_call))\n",
    "            else:\n",
    "                data_label_y.append(len(calls2use))\n",
    "            \n",
    "            data_filename.append(cur_filename)\n",
    "\n",
    "        data_dsMPS_x = np.array(data_dsMPS_x).astype(int8)\n",
    "        data_label_y = np.array(data_label_y).astype(int)\n",
    "\n",
    "        print(\"Saving file\" + out_dsMPS_data_test_file)\n",
    "        hf = h5py.File(out_dsMPS_data_test_file, \"w\")\n",
    "        hf.create_dataset('data_dsMPS_x',data=data_dsMPS_x)\n",
    "        hf.create_dataset('data_label_y',data=data_label_y)\n",
    "        hf.close()\n",
    "    else: \n",
    "        print(\"File (\" + out_dsMPS_data_test_file + \") already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c728683615649ae6024bdf2b54a6d3b0d13aadd4e771e3670de23ca9a3f30dd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
