{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import h5py\n",
    "import scipy.io\n",
    "import matlab_helpers as mh \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read unique files and create training and testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604 total unique files: ex = D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/vIHC_40ms/level65_dBspl_clean/Chut/sgRNN_Chut_2_Feb_07_2022_51861688_ms_101198_101787.mat\n",
      "snr_value=-20.0 with weight = 0.125\n",
      "snr_value=-15.0 with weight = 0.25\n",
      "snr_value=-10.0 with weight = 0.375\n",
      "snr_value=-5.0 with weight = 0.5\n",
      "snr_value=0.0 with weight = 0.625\n",
      "snr_value=5.0 with weight = 0.75\n",
      "snr_value=10.0 with weight = 0.875\n",
      "snr_value=inf with weight = 1.0\n",
      "Saving fileD:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_train_file.npz\n"
     ]
    }
   ],
   "source": [
    "from numpy import int8\n",
    "\n",
    "root_in_dir= 'D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/vIHC_40ms/'\n",
    "root_out_dir= 'D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/'\n",
    "clean_call_in_dir= root_in_dir + 'level65_dBspl_clean/'\n",
    "if not os.path.exists(root_out_dir):\n",
    "    os.makedirs(root_out_dir)\n",
    "\n",
    "out_sgRNN_data_train_file= root_out_dir + 'sgRNN_data_train_file.npz'\n",
    "out_sgRNN_data_train_listfile= root_out_dir + 'sgRNN_train_list.txt'\n",
    "out_sgRNN_data_test_listfile= root_out_dir + 'sgRNN_test_file.txt'\n",
    "\n",
    "valid_datadirs = ['Chut', 'HighWhistle', 'Rumble', 'Tchatter', 'Wheek', 'Whine']\n",
    "calls2use = ['Chut', 'Rumble', 'Wheek', 'Whine']\n",
    "\n",
    "files_all_calls_fullpath = []\n",
    "files_all_calls_nameonly = []\n",
    "for cur_call_dir in valid_datadirs:\n",
    "    cur_call_path = clean_call_in_dir + cur_call_dir + '/' \n",
    "    cur_dir_files = [f for f in os.listdir(cur_call_path) if os.path.isfile(os.path.join(cur_call_path, f))]\n",
    "    files_all_calls_fullpath = files_all_calls_fullpath + [cur_call_path + item for item in cur_dir_files]\n",
    "    files_all_calls_nameonly = files_all_calls_nameonly + [item for item in cur_dir_files]\n",
    "\n",
    "print(f\"{len(files_all_calls_fullpath)} total unique files: ex = {files_all_calls_fullpath[0]}\")\n",
    "\n",
    "training_files, testing_files, training_inds, testing_inds = \\\n",
    "    train_test_split(files_all_calls_fullpath, np.arange(len(files_all_calls_fullpath)), test_size= 0.25, random_state=1)\n",
    "\n",
    "with open(out_sgRNN_data_train_listfile, 'w') as f:\n",
    "    for line in training_files:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "with open(out_sgRNN_data_test_listfile, 'w') as f:\n",
    "    for line in testing_files:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "all_snrs= np.arange(-20, 11, 5)\n",
    "all_snrs= np.append(all_snrs, np.inf)\n",
    "snr_training_weights= np.flip(len(all_snrs)-np.arange(len(all_snrs)))/len(all_snrs)\n",
    "\n",
    "temp_data_sgRNN_x = []\n",
    "data_label_y = []\n",
    "data_filename = []\n",
    "\n",
    "pre_search_str = '_clean/'\n",
    "post_search_str = '/sgRNN_'\n",
    "for snr_value,cur_weight in zip(all_snrs, snr_training_weights):\n",
    "    print(f\"snr_value={snr_value} with weight = {cur_weight}\")\n",
    "    if np.isinf(snr_value):\n",
    "        cur_snr_in_dir= root_in_dir + 'level65_dBspl_clean/' \n",
    "    else:\n",
    "        cur_snr_in_dir= root_in_dir + 'level65_dBspl_SNR' + snr_value.astype('int').astype('str') + '_white/' \n",
    "\n",
    "    if cur_weight<1:\n",
    "        cur_snr_train_inds, _ = train_test_split(np.arange(len(training_inds)), test_size= 1-cur_weight, random_state=1)\n",
    "    else:\n",
    "        cur_snr_train_inds = training_inds\n",
    "\n",
    "    for ind in cur_snr_train_inds:\n",
    "        cur_file = files_all_calls_fullpath[ind]\n",
    "        cur_call = cur_file[cur_file.rfind(pre_search_str)+len(pre_search_str):cur_file.rfind(post_search_str)]\n",
    "        # print(f\"cur_file={cur_file} | cur_call={cur_call}\")\n",
    "        cur_filename= cur_snr_in_dir + cur_call + '/' + files_all_calls_nameonly[ind]\n",
    "\n",
    "        new_data = mh.loadmat(cur_filename)\n",
    "        temp_data_sgRNN_x.append(new_data[\"sgRNN_data\"][\"pow_dB\"])\n",
    "\n",
    "        if cur_call in calls2use:\n",
    "            data_label_y.append(calls2use.index(cur_call))\n",
    "        else:\n",
    "            data_label_y.append(len(calls2use))\n",
    "        \n",
    "        data_filename.append(cur_filename)\n",
    "\n",
    "        # print(cur_filename)\n",
    "\n",
    "num_seg = [item.shape for item in temp_data_sgRNN_x]\n",
    "\n",
    "if (not os.path.exists(out_sgRNN_data_train_file)):\n",
    "    print(\"Saving file\" + out_sgRNN_data_train_file)\n",
    "    data_sgRNN_x = np.empty(len(temp_data_sgRNN_x), object)\n",
    "    data_sgRNN_x[:] = temp_data_sgRNN_x\n",
    "    np.savez(out_sgRNN_data_train_file, data_sgRNN_x=data_sgRNN_x, data_label_y=data_label_y, data_filename=data_filename)\n",
    "else: \n",
    "    print(\"File (\" + out_sgRNN_data_train_file + \") already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr-20.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr-15.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr-10.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr-5.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr0.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr5.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snr10.npz, len=401\n",
      "Saving file D:/Dropbox/Python/MLmodels/Datasets/sgRNN_vIHC/PyData/sgRNN_data_test_snrInf.npz, len=401\n"
     ]
    }
   ],
   "source": [
    "for snr_value in all_snrs:\n",
    "    if np.isinf(snr_value):\n",
    "        cur_snr_in_dir= root_in_dir + 'level65_dBspl_clean/' \n",
    "    else:\n",
    "        cur_snr_in_dir= root_in_dir + 'level65_dBspl_SNR' + snr_value.astype('int').astype('str') + '_white/' \n",
    "\n",
    "    temp_data_sgRNN_x = []\n",
    "    data_label_y = []\n",
    "    data_filename = []\n",
    "\n",
    "    if np.isinf(snr_value):\n",
    "        out_sgRNN_data_test_file= root_out_dir + 'sgRNN_data_test_snrInf.npz'\n",
    "    else: \n",
    "        out_sgRNN_data_test_file= root_out_dir + 'sgRNN_data_test_snr' + str(np.int_(snr_value)) + '.npz'\n",
    "    # print(out_sgRNN_data_test_file)\n",
    "\n",
    "    if (not os.path.exists(out_sgRNN_data_test_file)):\n",
    "\n",
    "        for ind in testing_inds:\n",
    "            cur_file = files_all_calls_fullpath[ind]\n",
    "            cur_call = cur_file[cur_file.rfind(pre_search_str)+len(pre_search_str):cur_file.rfind(post_search_str)]\n",
    "            # print(f\"cur_file={cur_file} | cur_call={cur_call}\")\n",
    "            cur_filename= cur_snr_in_dir + cur_call + '/' + files_all_calls_nameonly[ind]\n",
    "\n",
    "            new_data = mh.loadmat(cur_filename)\n",
    "            temp_data_sgRNN_x.append(new_data[\"sgRNN_data\"][\"pow_dB\"])\n",
    "\n",
    "            if cur_call in calls2use:\n",
    "                data_label_y.append(calls2use.index(cur_call))\n",
    "            else:\n",
    "                data_label_y.append(len(calls2use))\n",
    "            \n",
    "            data_filename.append(cur_filename)\n",
    "\n",
    "        # data_sgRNN_x = np.array(data_sgRNN_x).astype(int8)\n",
    "        # data_label_y = np.array(data_label_y).astype(int)\n",
    "\n",
    "        data_sgRNN_x = np.empty(len(temp_data_sgRNN_x), object)\n",
    "        data_sgRNN_x[:] = temp_data_sgRNN_x\n",
    "        print(f\"Saving file {out_sgRNN_data_test_file}, len={len(data_sgRNN_x)}\")\n",
    "        np.savez(out_sgRNN_data_test_file, data_sgRNN_x=data_sgRNN_x, data_label_y=data_label_y, data_filename=data_filename)\n",
    "\n",
    "        # print(\"Saving file\" + out_sgRNN_data_test_file)\n",
    "        # hf = h5py.File(out_sgRNN_data_test_file, \"w\")\n",
    "        # hf.create_dataset('data_sgRNN_x',data=data_sgRNN_x)\n",
    "        # hf.create_dataset('data_label_y',data=data_label_y)\n",
    "        # hf.close()\n",
    "    else: \n",
    "        print(\"File (\" + out_sgRNN_data_test_file + \") already exists\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
